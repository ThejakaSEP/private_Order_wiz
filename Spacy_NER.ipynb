{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Without Fine tuning\n",
        "Only adding the entity to the ruler"
      ],
      "metadata": {
        "id": "MoF_eJznq_RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "xpv5WPKbtMQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "cNnv8VLRtMNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruler = nlp.add_pipe('entity_ruler')"
      ],
      "metadata": {
        "id": "ZUD0geUir30i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [\n",
        "    #Menu Items\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"cappuccino\"},\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"cappuccinos\"},\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"latte\"},\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"lattes\"},\n",
        "    #Sizes\n",
        "    {\"label\":\"SIZE\",\"pattern\":\"small\"},\n",
        "    {\"label\":\"SIZE\",\"pattern\":\"medium\"},\n",
        "    {\"label\":\"SIZE\",\"pattern\":\"large\"},\n",
        "    #Cardinals\n",
        "    {\"label\":\"CARDINAL\",\"pattern\":\"a\"},\n",
        "]"
      ],
      "metadata": {
        "id": "1hNbldzgss0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "Q3U07c-js6W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing before fine tuning\n",
        "sentence = \"I'd like a medium Cappuccino with extra foam and two small Lattes.\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOG6C32TFGwq",
        "outputId": "dcab6ff0-02be-4e04-a248-8db42782ba86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('a', 'CARDINAL'), ('medium', 'SIZE'), ('cappuccino', 'BEVERAGE'), ('two', 'CARDINAL'), ('small', 'SIZE'), ('lattes', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# More complex sentence\n",
        "sentence = \"I'd like a large Cappuccino with extra foam, two medium Lattes and a small cappuccino.\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKFgT3TZs90T",
        "outputId": "f7ff152c-5c4c-4405-c379-03d227915f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('a', 'CARDINAL'), ('large', 'SIZE'), ('cappuccino', 'BEVERAGE'), ('two', 'CARDINAL'), ('medium', 'SIZE'), ('lattes', 'BEVERAGE'), ('a', 'CARDINAL'), ('small', 'SIZE'), ('cappuccino', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# More complex sentence\n",
        "sentence = \"I'd like a large Cappuccino and two small cappuccinos.\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-KmaxNweDbV",
        "outputId": "13274d73-3418-40e9-9caf-00badb01d7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('a', 'CARDINAL'), ('large', 'SIZE'), ('cappuccino', 'BEVERAGE'), ('two', 'CARDINAL'), ('small', 'SIZE'), ('cappuccinos', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# order_dict = {'Item':[],\n",
        "#               'Size':[],\n",
        "#               'Quantity':[]}\n",
        "\n",
        "# for ent in doc.ents:"
      ],
      "metadata": {
        "id": "zhe8bpgtf4Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the function for setting up Spacy nlp object\n",
        "def setup_spacy_ner(pattern):\n",
        "  import spacy\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  ruler = nlp.add_pipe('entity_ruler')\n",
        "  ruler.add_patterns(patterns)\n",
        "  return nlp"
      ],
      "metadata": {
        "id": "yoBw7lsf3sfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the patterns according to Menu\n",
        "\n",
        "patterns = [\n",
        "    #Menu Items\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"cappuccino\"},\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"cappuccinos\"},\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"latte\"},\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"lattes\"},\n",
        "    #Sizes\n",
        "    {\"label\":\"SIZE\",\"pattern\":\"small\"},\n",
        "    {\"label\":\"SIZE\",\"pattern\":\"medium\"},\n",
        "    {\"label\":\"SIZE\",\"pattern\":\"large\"},\n",
        "    #Cardinals\n",
        "    {\"label\":\"CARDINAL\",\"pattern\":\"a\"},\n",
        "]\n",
        "\n",
        "# Initiate nlp object of Spacy with the patterns\n",
        "nlp = setup_spacy_ner(patterns)"
      ],
      "metadata": {
        "id": "ivT_pcAW4EHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the nlp object's ner function\n",
        "\n",
        "sentence = \"I'd like a large Cappuccino with extra foam, two medium Lattes and a small cappuccino.\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfUBrth_4V6l",
        "outputId": "a98040ee-bb4d-408f-c34a-9f89a1fb038c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('a', 'CARDINAL'), ('large', 'SIZE'), ('cappuccino', 'BEVERAGE'), ('two', 'CARDINAL'), ('medium', 'SIZE'), ('lattes', 'BEVERAGE'), ('a', 'CARDINAL'), ('small', 'SIZE'), ('cappuccino', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to log in the order details\n",
        "order_dict = {'Item':[],\n",
        "              'Size':[],\n",
        "              'Quantity':[]}\n",
        "\n",
        "def insert_order(sentence):\n",
        "  doc = nlp(sentence.lower())\n",
        "  for i,ent in enumerate(doc.ents):\n",
        "    if ent.label_ == 'BEVERAGE':\n",
        "      order_dict['Item'].append(ent.text)\n",
        "    elif ent.label_ == 'SIZE':\n",
        "      order_dict['Size'].append(ent.text)\n",
        "    elif ent.label_ == 'CARDINAL':\n",
        "      if ent.text == 'a' or ent.text == 'an':\n",
        "        order_dict['Quantity'].append('One')\n",
        "      else:\n",
        "        order_dict['Quantity'].append(ent.text)\n",
        "\n",
        "  return order_dict"
      ],
      "metadata": {
        "id": "009Hrz2e94eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insert_order(\"I'd like a Cappuccino with extra foam, two medium Lattes and a small cappuccino.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfVE_LLJC9VC",
        "outputId": "6e6978c4-4f8c-43e3-96ed-00939d1d0286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Item': ['cappuccino', 'lattes', 'cappuccino'],\n",
              " 'Size': ['medium', 'small'],\n",
              " 'Quantity': ['One', 'two', 'One']}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning (100 sample data)"
      ],
      "metadata": {
        "id": "NeYVYBAZtDBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A2LIwoHtml7"
      },
      "outputs": [],
      "source": [
        "# Load pre-existing spacy model\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the entity to the ruler before fine tuning\n",
        "ruler = nlp.add_pipe('entity_ruler')\n",
        "\n",
        "patterns = [\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"Cappuccino\"}\n",
        "]\n",
        "\n",
        "ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "eNxNR3TBttaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing before fine tuning\n",
        "doc = nlp(\"I'd like a large Cappuccino with extra foam.\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo0akVMht_6m",
        "outputId": "c6c4f227-80c5-4630-99fe-cef6aacb9c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Cappuccino', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it is incorrectly categorizing \"Cappuccino\" as a \"PERSON\""
      ],
      "metadata": {
        "id": "s1HXHRc6uFiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the pipeline component\n",
        "ner=nlp.get_pipe(\"ner\")"
      ],
      "metadata": {
        "id": "RekeQmUettEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_entity_output(sentence, word, entity_name):\n",
        "    start_index = sentence.index(word)\n",
        "    end_index = start_index + len(word) - 1\n",
        "    output = {\n",
        "        \"entities\": [\n",
        "            (start_index, end_index, entity_name)\n",
        "        ]\n",
        "    }\n",
        "    return (sentence, output)"
      ],
      "metadata": {
        "id": "RjCJtUOltml7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('custom_dataset_Cappuccino.txt', encoding='UTF-8') as file:\n",
        "    # lines = [line.rstrip() for line in file]\n",
        "    lines = [line.strip('\"').rstrip().rstrip('\"') for line in file]"
      ],
      "metadata": {
        "id": "4W4eidENtml8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = []\n",
        "\n",
        "for line in lines:\n",
        "  train_set.append(generate_entity_output(line,\"Cappuccino\",\"BEVERAGE\"))"
      ],
      "metadata": {
        "id": "ImfoiiwJtml8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import training\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in train_set:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])"
      ],
      "metadata": {
        "id": "db0M7qt1tml8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "1xUNONf3tml8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7a33bc-b320-4fcd-eec3-31ea8cde9928",
        "id": "ZVkfRRuxtml8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"I'll have a small Cappuccino, please.\", {'entities': [(18, 27, 'BEVERAGE')]})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "from spacy.training import Example\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(100):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(train_set)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(train_set, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        example = []\n",
        "        # Update the model with iterating each text\n",
        "        for i in range(len(texts)):\n",
        "            doc = nlp.make_doc(texts[i])\n",
        "            example.append(Example.from_dict(doc, annotations[i]))\n",
        "        \n",
        "        # Update the model\n",
        "        nlp.update(example, drop=0.5, losses=losses)\n",
        "\n",
        "        print(\"Losses\", losses)"
      ],
      "metadata": {
        "id": "tyzUzb95tml8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"I'd like a large Cappuccino with extra foam.\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454fabee-d3d5-4dea-d863-cebfa346029a",
        "id": "udYScehztml8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Cappuccino', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune (50 sample data)"
      ],
      "metadata": {
        "id": "izzOF4-Wu_u8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS8pfdScw-Mm"
      },
      "outputs": [],
      "source": [
        "# Load pre-existing spacy model\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the entity to the ruler before fine tuning\n",
        "ruler = nlp.add_pipe('entity_ruler')\n",
        "\n",
        "patterns = [\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"Cappuccino\"}\n",
        "]\n",
        "\n",
        "ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "-ZKiuet0w-Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing before fine tuning\n",
        "doc = nlp(\"I'd like a large Cappuccino with extra foam.\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a8383d-dce6-4513-9017-93d82dba1730",
        "id": "RqDqR59nw-Mn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Cappuccino', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it is incorrectly categorizing \"Cappuccino\" as a \"PERSON\""
      ],
      "metadata": {
        "id": "Zut4eA5kw-Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the pipeline component\n",
        "ner=nlp.get_pipe(\"ner\")"
      ],
      "metadata": {
        "id": "c1N0jFUiw-Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_entity_output(sentence, word, entity_name):\n",
        "    start_index = sentence.index(word)\n",
        "    end_index = start_index + len(word) - 1\n",
        "    output = {\n",
        "        \"entities\": [\n",
        "            (start_index, end_index, entity_name)\n",
        "        ]\n",
        "    }\n",
        "    return (sentence, output)"
      ],
      "metadata": {
        "id": "7_fgK0Pbw-Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('custom_dataset_Cappuccino.txt', encoding='UTF-8') as file:\n",
        "    # lines = [line.rstrip() for line in file]\n",
        "    lines = [line.strip('\"').rstrip().rstrip('\"') for line in file]"
      ],
      "metadata": {
        "id": "BWP7Vgfiw-Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = []\n",
        "\n",
        "for line in lines[:10]:\n",
        "  train_set.append(generate_entity_output(line,\"Cappuccino\",\"BEVERAGE\"))"
      ],
      "metadata": {
        "id": "dAziKHAww-Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J8AMQVWxGFP",
        "outputId": "13aaf51b-a747-489d-b165-aa8598d221e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import training\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in train_set:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])"
      ],
      "metadata": {
        "id": "9caPbgRgw-Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "XBK8CrV6w-Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6deb9d21-0837-4b20-93ad-4fbf0c8e2b21",
        "id": "jOxQAK1uw-Mo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Can I order a small Cappuccino with chocolate sprinkles?',\n",
              "  {'entities': [(20, 29, 'BEVERAGE')]}),\n",
              " ('Could I get a medium Cappuccino with almond milk?',\n",
              "  {'entities': [(21, 30, 'BEVERAGE')]}),\n",
              " ('Can I get a medium Cappuccino to go?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " ('Could I get a small Cappuccino with a sprinkle of cocoa powder?',\n",
              "  {'entities': [(20, 29, 'BEVERAGE')]}),\n",
              " (\"I'd like a medium Cappuccino with a dash of vanilla syrup.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " ('May I have a large Cappuccino with whipped cream, please?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " (\"I'll take a large Cappuccino with a shot of caramel.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " (\"I'd like a large Cappuccino with extra foam.\",\n",
              "  {'entities': [(17, 26, 'BEVERAGE')]}),\n",
              " ('May I have a small Cappuccino with cinnamon on top?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " (\"I'll have a small Cappuccino, please.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]})]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "from spacy.training import Example\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(100):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(train_set)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(train_set, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        example = []\n",
        "        # Update the model with iterating each text\n",
        "        for i in range(len(texts)):\n",
        "            doc = nlp.make_doc(texts[i])\n",
        "            example.append(Example.from_dict(doc, annotations[i]))\n",
        "        \n",
        "        # Update the model\n",
        "        nlp.update(example, drop=0.5, losses=losses)\n",
        "\n",
        "        # print(\"Losses\", losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTomneRow-Mo",
        "outputId": "31bd4b75-745c-4640-e575-82776d839b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Could I get a medium Cappuccino with almond milk?\" with entities \"[(21, 30, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I'll have a small Cappuccino, please.\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I'd like a medium Cappuccino with a dash of vanill...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"May I have a large Cappuccino with whipped cream, ...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I'll take a large Cappuccino with a shot of carame...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I'd like a large Cappuccino with extra foam.\" with entities \"[(17, 26, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"May I have a small Cappuccino with cinnamon on top...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Can I order a small Cappuccino with chocolate spri...\" with entities \"[(20, 29, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Can I get a medium Cappuccino to go?\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Could I get a small Cappuccino with a sprinkle of ...\" with entities \"[(20, 29, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"May I have a medium Cappuccino with a dollop of whipped cream?\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9004ee-8877-47e1-af0e-dfb1393dcd6e",
        "id": "F6tjsufhw-Mp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Cappuccino', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding another Entity (Size)"
      ],
      "metadata": {
        "id": "IsJNTHFjyIuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_xNpjeK7fwS"
      },
      "outputs": [],
      "source": [
        "# Load pre-existing spacy model\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the entity to the ruler before fine tuning\n",
        "ruler = nlp.add_pipe('entity_ruler')\n",
        "\n",
        "patterns = [\n",
        "    {\"label\":\"BEVERAGE\",\"pattern\":\"cappuccino\"},\n",
        "    \n",
        "]\n",
        "\n",
        "ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "0QgjA0c37fwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing before fine tuning\n",
        "sentence = \"I'd like a medium Cappuccino with extra foam and a small Latte.\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fd71c5-a44e-4d19-d4f4-9910d522f6fb",
        "id": "ok6J7X1Y7fwf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('cappuccino', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"I'd like a large Cappuccino with extra foam and a Latte.\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BuCONbLE5LV",
        "outputId": "baf19251-b090-439f-9f47-b3e19d762147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Cappuccino', 'PERSON'), ('Latte', 'ORG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it is incorrectly categorizing \"Cappuccino\" as a \"PERSON\""
      ],
      "metadata": {
        "id": "1TS4fIr27fwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the pipeline component\n",
        "ner=nlp.get_pipe(\"ner\")"
      ],
      "metadata": {
        "id": "IeP4VadD7fwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_entity_output(sentence, word, entity_name):\n",
        "    start_index = sentence.index(word)\n",
        "    end_index = start_index + len(word) - 1\n",
        "    output = {\n",
        "        \"entities\": [\n",
        "            (start_index, end_index, entity_name)\n",
        "        ]\n",
        "    }\n",
        "    return (sentence, output)"
      ],
      "metadata": {
        "id": "xi_iWZTo7fwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('custom_dataset_Cappuccino_Latte.txt', encoding='UTF-8') as file:\n",
        "    # lines = [line.rstrip() for line in file]\n",
        "    lines = [line.strip('\"').rstrip().rstrip('\"').lower() for line in file]"
      ],
      "metadata": {
        "id": "E7S5Tjtr7fwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sn_shpUz9GTy",
        "outputId": "026275a4-fcaf-4df7-d61c-05ea185dcefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'can i get a medium cappuccino and a large latte to go?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = []\n",
        "\n",
        "for line in lines:\n",
        "  train_set.append(generate_entity_output(line,\"cappuccino\",\"BEVERAGE\"))"
      ],
      "metadata": {
        "id": "jwr6WnFs7fwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e45a5d-a99b-40ad-d4f0-a1ad57603ce9",
        "id": "eLTCqmAx7fwg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import training\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in train_set:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])"
      ],
      "metadata": {
        "id": "br-0mBt47fwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "6jIwrbes7fwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728441c3-e116-4638-b5c4-6339dae78b80",
        "id": "TsYLMN_y7fwg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"i'll have a small cappuccino and a medium latte, please.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " ('can i get a medium cappuccino and a large latte to go?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " (\"i'd like a large cappuccino and a small latte with an extra shot of espresso.\",\n",
              "  {'entities': [(17, 26, 'BEVERAGE')]}),\n",
              " ('may i have a small cappuccino and a medium latte with almond milk, please?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " ('could i get a medium cappuccino and a large latte with a dusting of cocoa powder?',\n",
              "  {'entities': [(21, 30, 'BEVERAGE')]}),\n",
              " (\"i'll take a large cappuccino and a small latte with caramel syrup.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " ('can i order a small cappuccino and a medium latte with whipped cream?',\n",
              "  {'entities': [(20, 29, 'BEVERAGE')]}),\n",
              " (\"i'd like a medium cappuccino and a large latte with soy milk, please.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " ('may i have a small cappuccino and a medium latte with a sprinkle of cinnamon?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " ('could i get a medium cappuccino and a large latte with a shot of vanilla syrup?',\n",
              "  {'entities': [(21, 30, 'BEVERAGE')]})]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "from spacy.training import Example\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(100):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(train_set)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(train_set, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        example = []\n",
        "        # Update the model with iterating each text\n",
        "        for i in range(len(texts)):\n",
        "            doc = nlp.make_doc(texts[i])\n",
        "            example.append(Example.from_dict(doc, annotations[i]))\n",
        "        \n",
        "        # Update the model\n",
        "        nlp.update(example, drop=0.5, losses=losses)\n",
        "\n",
        "        # print(\"Losses\", losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a069981-cb88-41c0-f5fa-7ae40c0c4cd7",
        "id": "aRLObZTg7fwh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i order a small cappuccino and a medium latte ...\" with entities \"[(20, 29, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"may i have a small cappuccino and a medium latte w...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"could i get a medium cappuccino and a large latte ...\" with entities \"[(21, 30, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'd like a large cappuccino and a small latte with...\" with entities \"[(17, 26, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll have a small cappuccino and a medium latte wi...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'd like a medium cappuccino and a large latte wit...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll take a large cappuccino and a small latte wit...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i get a medium cappuccino and a large latte wi...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll have a small cappuccino and a medium latte, p...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i get a medium cappuccino and a large latte to...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "sentence = \"May I have a medium Cappuccino and a  Latte with a dollop of whipped cream Amazon?\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fce250-f906-497f-87c4-94331edd3f17",
        "id": "soE3tjWk7fwh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('medium', 'SIZE'), ('cappuccino', 'BEVERAGE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the order list\n",
        "order = {\"Item\":[],\n",
        "         \"Size\":[]}\n",
        "\n",
        "input_sentence = \"\"\n",
        "\n",
        "while input_sentence != 'confirm':\n",
        "  input_sentence = input(\"What would you like to order? : \")\n",
        "  doc = nlp(input_sentence.lower())\n",
        "\n",
        "  for i,ent in enumerate(doc.ents):\n",
        "    if ent.label_ == \"BEVERAGE\":\n",
        "      order['Item'].append(ent.text)\n",
        "      if i!=0 and doc.ents[i-1].label_=='SIZE':\n",
        "        order['Size'].append(doc.ents[i-1].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcIYHl1Z8O5R",
        "outputId": "2ecb2db8-beca-46d9-cd47-95296b5427b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What would you like to order? : small latte\n",
            "What would you like to order? : confirm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "order"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrNdrcfR-1sG",
        "outputId": "36cdfe45-9886-4950-c6ee-a6b401fbbc03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Item': [], 'Size': []}"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_7ZXGF1CfxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without Pattern (Add Entity)\n",
        "Only adding the entity to the ruler"
      ],
      "metadata": {
        "id": "R2Y74o06xge2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "eQqNwo6ixge2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ner = nlp.get_pipe('ner')\n",
        "ner.add_label('BEVERAGE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUi9KQg1xge3",
        "outputId": "27d8ad93-9ea7-4637-8b6c-afc24f2b17f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ruler = nlp.add_pipe('entity_ruler')"
      ],
      "metadata": {
        "id": "7g0EbHQsxge3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# patterns = [\n",
        "#     {\"label\":\"BEVERAGE\",\"pattern\":\"cappuccino\"}\n",
        "# ]"
      ],
      "metadata": {
        "id": "PSr92lRqxge3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "wHlaudUWxge3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing before fine tuning\n",
        "sentence = \"I'd like a medium Cappuccino with extra foam and a small Latte.\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e63fc0e-7364-4467-f449-19b95d5ffbca",
        "id": "jXVmNWOpxge3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "doc = nlp(\"I'd like a large Cappuccino with extra foam and a Latte.\")\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795465a4-ad35-4437-d17e-11a27edaf25e",
        "id": "GRvXWNQ1xge3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities [('Cappuccino', 'PERSON'), ('Latte', 'ORG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it is incorrectly categorizing \"Cappuccino\" as a \"PERSON\""
      ],
      "metadata": {
        "id": "-0Gp6T2myQcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_entity_output(sentence, word, entity_name):\n",
        "    start_index = sentence.index(word)\n",
        "    end_index = start_index + len(word) - 1\n",
        "    output = {\n",
        "        \"entities\": [\n",
        "            (start_index, end_index, entity_name)\n",
        "        ]\n",
        "    }\n",
        "    return (sentence, output)"
      ],
      "metadata": {
        "id": "63RehTm5yQcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('custom_dataset_Cappuccino_Latte.txt', encoding='UTF-8') as file:\n",
        "    # lines = [line.rstrip() for line in file]\n",
        "    lines = [line.strip('\"').rstrip().rstrip('\"').lower() for line in file]"
      ],
      "metadata": {
        "id": "CXDPWrMbyQcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ee60e8e-5c31-41ed-8924-f8482745acaf",
        "id": "q1m5YFLHyQcE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'can i get a medium cappuccino and a large latte to go?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = []\n",
        "\n",
        "for line in lines:\n",
        "  train_set.append(generate_entity_output(line,\"cappuccino\",\"BEVERAGE\"))"
      ],
      "metadata": {
        "id": "Uk9mIdhKyQcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea79f53-b835-4d21-bf35-4a8bb7f2921c",
        "id": "MUYmBpN4yQcE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import training\n",
        "# Adding labels to the `ner`\n",
        "\n",
        "for _, annotations in train_set:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    ner.add_label(ent[2])"
      ],
      "metadata": {
        "id": "1A7lLNxbyQcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "s0yhZ_1gyQcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed26b3d3-6809-483f-a599-9dc90117cf4c",
        "id": "xVUw1QSJyQcF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"i'll have a small cappuccino and a medium latte, please.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " ('can i get a medium cappuccino and a large latte to go?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " (\"i'd like a large cappuccino and a small latte with an extra shot of espresso.\",\n",
              "  {'entities': [(17, 26, 'BEVERAGE')]}),\n",
              " ('may i have a small cappuccino and a medium latte with almond milk, please?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " ('could i get a medium cappuccino and a large latte with a dusting of cocoa powder?',\n",
              "  {'entities': [(21, 30, 'BEVERAGE')]}),\n",
              " (\"i'll take a large cappuccino and a small latte with caramel syrup.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " ('can i order a small cappuccino and a medium latte with whipped cream?',\n",
              "  {'entities': [(20, 29, 'BEVERAGE')]}),\n",
              " (\"i'd like a medium cappuccino and a large latte with soy milk, please.\",\n",
              "  {'entities': [(18, 27, 'BEVERAGE')]}),\n",
              " ('may i have a small cappuccino and a medium latte with a sprinkle of cinnamon?',\n",
              "  {'entities': [(19, 28, 'BEVERAGE')]}),\n",
              " ('could i get a medium cappuccino and a large latte with a shot of vanilla syrup?',\n",
              "  {'entities': [(21, 30, 'BEVERAGE')]})]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "from spacy.training import Example\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(100):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(train_set)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(train_set, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        example = []\n",
        "        # Update the model with iterating each text\n",
        "        for i in range(len(texts)):\n",
        "            doc = nlp.make_doc(texts[i])\n",
        "            example.append(Example.from_dict(doc, annotations[i]))\n",
        "        \n",
        "        # Update the model\n",
        "        nlp.update(example, drop=0.5, losses=losses)\n",
        "\n",
        "        # print(\"Losses\", losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7ea892-0f23-4533-f202-5e3b78733821",
        "id": "F-50AcBIyQcF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"may i have a small cappuccino and a medium latte w...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"could i get a medium cappuccino and a large latte ...\" with entities \"[(21, 30, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll have a small cappuccino and a medium latte wi...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i order a small cappuccino and a medium latte ...\" with entities \"[(20, 29, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'd like a medium cappuccino and a large latte wit...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll have a small cappuccino and a medium latte, p...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll take a large cappuccino and a small latte wit...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i get a medium cappuccino and a large latte to...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i get a medium cappuccino and a large latte wi...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'd like a large cappuccino and a small latte with...\" with entities \"[(17, 26, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "sentence = \"May I have a medium Cappuccino and a  Latte with a dollop of whipped cream Amazon?\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f5d912-7f12-42df-e00b-88da0ac95178",
        "id": "U9ovkjrgyQcF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New way (Custom NER)"
      ],
      "metadata": {
        "id": "9j-XhKA_yq1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "d_36XL-E_dft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_spacy(TRAIN_DATA,iterations):\n",
        "  nlp = spacy.blank(\"en\")\n",
        "  ner = nlp.create_pipe(\"ner\")\n",
        "  ner.add_label(\"BEVERAGE\")\n",
        "  nlp.add_pipe(ner, name=\"beverage_ner\")\n",
        "  other_pipes = [pipe for pipe in nlp.pipe_names if pipe!=\"beverage_ner\"]\n",
        "  \n",
        "  with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(iterations):\n",
        "        print(f\"Starting iteration {str(itn)}\")\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        for text, annotations in TRAIN_DATA:\n",
        "          nlp.update([text],\n",
        "                    [annotations],\n",
        "                    drop=0.2,\n",
        "                    sgd=optimizer,\n",
        "                    losses=losses)\n",
        "        print(losses)\n",
        "  return (nlp)\n",
        "\n",
        "\n",
        "        # random.shuffle(train_set)\n",
        "        # losses = {}\n",
        "        # # batch up the examples using spaCy's minibatch\n",
        "        # batches = minibatch(train_set, size=compounding(4.0, 32.0, 1.001))\n",
        "        # for batch in batches:\n",
        "        #     texts, annotations = zip(*batch)\n",
        "        #     example = []\n",
        "        #     # Update the model with iterating each text\n",
        "        #     for i in range(len(texts)):\n",
        "        #         doc = nlp.make_doc(texts[i])\n",
        "        #         example.append(Example.from_dict(doc, annotations[i]))\n",
        "            \n",
        "        #     # Update the model\n",
        "        #     nlp.update(example, drop=0.5, losses=losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "SaULP3cO_pFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spacy(train_set,10)"
      ],
      "metadata": {
        "id": "azKnJ8yuLuVB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "d462e950-b156-42e6-cf3e-4b5ab9675151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-84fd3f8a9319>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-7aa319cdf28e>\u001b[0m in \u001b[0;36mtrain_spacy\u001b[0;34m(TRAIN_DATA, iterations)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BEVERAGE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"beverage_ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mother_pipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpipe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"beverage_ner\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.ner.EntityRecognizer object at 0x7fc30d620120> (name: 'beverage_ner').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DNZSrtGN0rp",
        "outputId": "c4942eca-a30e-4932-ce72-6d93a5c66f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"i'd like a large cappuccino and a small latte with coconut milk, please.\",\n",
              " {'entities': [(17, 26, 'BEVERAGE')]})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Loading Data"
      ],
      "metadata": {
        "id": "0xW-BBQCH1KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_entity_output(sentence, word, entity_name):\n",
        "    start_index = sentence.index(word)\n",
        "    end_index = start_index + len(word) - 1\n",
        "    output = {\n",
        "        \"entities\": [\n",
        "            (start_index, end_index, entity_name)\n",
        "        ]\n",
        "    }\n",
        "    return (sentence, output)"
      ],
      "metadata": {
        "id": "_eBWWfazJXD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gck-eNucJYfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('custom_dataset_Cappuccino_Latte.txt', encoding='UTF-8') as file:\n",
        "    # lines = [line.rstrip() for line in file]\n",
        "    lines = [line.strip('\"').rstrip().rstrip('\"').lower() for line in file]"
      ],
      "metadata": {
        "id": "y3YcJF0TKcYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = []\n",
        "\n",
        "for line in lines:\n",
        "  train_set.append(generate_entity_output(line,\"cappuccino\",\"BEVERAGE\"))"
      ],
      "metadata": {
        "id": "GpBT7nr6KcYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import training\n",
        "# Adding labels to the `ner`\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.create_pipe(\"ner\")\n",
        "\n",
        "for _, annotations in train_set:\n",
        "  for ent in annotations.get(\"entities\"):\n",
        "    print(ent[2])\n",
        "    ner.add_label(ent[2])"
      ],
      "metadata": {
        "id": "Wc80XC7VKcYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable pipeline components you dont need to change\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "metadata": {
        "id": "pa1UVJnsKcYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import requirements\n",
        "import random\n",
        "from spacy.util import minibatch, compounding\n",
        "from pathlib import Path\n",
        "from spacy.training import Example\n",
        "\n",
        "# TRAINING THE MODEL\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "\n",
        "  # Training for 30 iterations\n",
        "  for iteration in range(100):\n",
        "\n",
        "    # shuufling examples  before every iteration\n",
        "    random.shuffle(train_set)\n",
        "    losses = {}\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(train_set, size=compounding(4.0, 32.0, 1.001))\n",
        "    for batch in batches:\n",
        "        texts, annotations = zip(*batch)\n",
        "        example = []\n",
        "        # Update the model with iterating each text\n",
        "        for i in range(len(texts)):\n",
        "            doc = nlp.make_doc(texts[i])\n",
        "            example.append(Example.from_dict(doc, annotations[i]))\n",
        "        \n",
        "        # Update the model\n",
        "        nlp.update(example, drop=0.5, losses=losses)\n",
        "\n",
        "        # print(\"Losses\", losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1931d561-81a2-45e4-ad47-4dc0a96ebfd9",
        "id": "qdc1uLyxKcYs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"could i get a medium cappuccino and a large latte ...\" with entities \"[(21, 30, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'd like a large cappuccino and a small latte with...\" with entities \"[(17, 26, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"may i have a small cappuccino and a medium latte w...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'd like a medium cappuccino and a large latte wit...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll have a small cappuccino and a medium latte, p...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i get a medium cappuccino and a large latte wi...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll take a large cappuccino and a small latte wit...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i order a small cappuccino and a medium latte ...\" with entities \"[(20, 29, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"i'll have a small cappuccino and a medium latte wi...\" with entities \"[(18, 27, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"can i get a medium cappuccino and a large latte to...\" with entities \"[(19, 28, 'BEVERAGE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "sentence = \"May I have a medium Cappuccino and a  Latte with a dollop of whipped cream Amazon?\"\n",
        "doc = nlp(sentence.lower())\n",
        "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb9a3dd-72de-4d81-f4fc-1ef1dcafeb47",
        "id": "ou_ir4DEKcYs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tDVj_l6RMWHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}