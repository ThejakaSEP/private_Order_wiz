{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The approach\n",
        "\n",
        "The focus is to train a spacy model to recognize an entity which has size, item and quantity in it. This way we can extract multiple entities if the user request many items at once. \n",
        "\n",
        "Once that is done, we will be using spacy patterns to extract the size, item and quanitity seperately from each entity. "
      ],
      "metadata": {
        "id": "TNyAKZXsrKlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 . Loading Dataset"
      ],
      "metadata": {
        "id": "9jzXG66nqxQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DgOLfAcHg8Sl"
      },
      "outputs": [],
      "source": [
        "with open(\"new_entity_format.txt\") as file:\n",
        "    # if not line.isspace():\n",
        "      lines = [line.lstrip().rstrip() for line in file if not line.isspace()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZH9jb8bg_LS",
        "outputId": "62c10b0f-4334-4598-8e5b-f3552115dedf"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'd like a small coffee and a medium latte, please.\",\n",
              " '[(a small coffee), (a medium latte)]',\n",
              " 'Can I get a large smoothie and two small coffees to go?',\n",
              " '[(a large smoothie), (two small coffees)]',\n",
              " \"I'll have a medium latte and a small smoothie.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 . Preprocessing dataset\n",
        "Note : Get the above list to a Spacy friendly training dataset format"
      ],
      "metadata": {
        "id": "Eo3WdPOYoTeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "sentences = []\n",
        "\n",
        "for i,line in enumerate(lines):\n",
        "  if i%2 == 0:\n",
        "    sentences.append(line)\n",
        "  else:\n",
        "    entities.append(line)"
      ],
      "metadata": {
        "id": "8vJCztx1q-9q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,entity in enumerate(entities):\n",
        "  entities[i] = entities[i].strip(\"[]\").split(\",\")\n",
        "  entities[i] = [item.strip(\"() \").lower() for item in entities[i]]"
      ],
      "metadata": {
        "id": "eavKu8Ure-np"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_entity_output(sentence, index, entity_name):\n",
        "    entity_list = []\n",
        "    for j,entity in enumerate(entities[index]):\n",
        "      start_index = sentence.index(entity)\n",
        "      end_index = start_index + len(entity) - 1\n",
        "      entity_list.append((start_index, end_index, entity_name))\n",
        "\n",
        "    output = {\n",
        "        \"entities\": entity_list\n",
        "    }\n",
        "    return (sentence, output)"
      ],
      "metadata": {
        "id": "8DddeNLaq-5w"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = []\n",
        "\n",
        "for i,sentence in enumerate(sentences):\n",
        "  train_set.append(generate_entity_output(sentence,i,\"Entity_Item\"))"
      ],
      "metadata": {
        "id": "xutg5-pFq-3q"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m-08nhAcnPN",
        "outputId": "8399e2d8-d431-47fc-a46a-4a319400611f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"I'd like a small coffee and a medium latte, please.\",\n",
              " {'entities': [(9, 22, 'Entity_Item'), (28, 41, 'Entity_Item')]})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zNctcTFmrZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 . Training Spacy"
      ],
      "metadata": {
        "id": "ZnJ5TxzVq5ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "db = DocBin() # create a DocBin object\n",
        "for text, annot in tqdm(train_set): # data in previous format\n",
        "    doc = nlp.make_doc(text) # create doc object from text\n",
        "    ents = []\n",
        "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "        if span is None:\n",
        "            print(\"Skipping entity\")\n",
        "        else:\n",
        "            ents.append(span)\n",
        "    doc.ents = ents # label the text with the ents\n",
        "    db.add(doc)\n",
        "db.to_disk(\"./train.spacy\") # save the docbin object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4toKkrBhVyu",
        "outputId": "38dda7ff-bb72-40b9-e683-6ac52746906c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 2383.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ6bWjlnlYqJ",
        "outputId": "d14d5d15-835b-4e52-e6a4-434a6b419529"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-31 02:23:36.384711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdlUM4Iinhgo",
        "outputId": "7e9e8287-c5ac-4d02-c274-739f856c87e4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-31 02:25:37.437793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-05-31 02:25:41,804] [INFO] Set up nlp object from config\n",
            "[2023-05-31 02:25:41,829] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2023-05-31 02:25:41,836] [INFO] Created vocabulary\n",
            "[2023-05-31 02:25:42,801] [WARNING] [W112] The model specified to use for initial vectors (en_core_web_sm) has no vectors. This is almost certainly a mistake.\n",
            "[2023-05-31 02:25:42,804] [INFO] Added vectors: en_core_web_sm\n",
            "[2023-05-31 02:25:42,806] [INFO] Finished initializing nlp object\n",
            "[2023-05-31 02:25:43,324] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     43.83    0.00    0.00    0.00    0.00\n",
            " 45     200          0.82    468.42  100.00  100.00  100.00    1.00\n",
            "101     400          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "167     600          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "250     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "350    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "450    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "606    1400          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "806    1600          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "1006    1800          0.00      0.00  100.00  100.00  100.00    1.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 . Load Trained model\n"
      ],
      "metadata": {
        "id": "7nj7CDVbntss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp1 = spacy.load(r\"./output/model-best\") #load the best model\n",
        "doc = nlp1(\"I need two large lattes and a medium coffee.\") # input sample text\n",
        "doc.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OknO2FcBppLw",
        "outputId": "1345daa2-bf6e-4f10-82d7-97502b54b435"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(two large, a medium)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afLp3pMPq964"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}